{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14811253,"sourceType":"datasetVersion","datasetId":9471237}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI Paper Summarizer & Q&A System\n\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q transformers datasets accelerate peft bitsandbytes\n!pip install -q rouge-score nltk evaluate\n!pip install -q fastapi uvicorn pyngrok\n!pip install -q PyPDF2 pypdf pyyaml python-dotenv\n\nprint(\"✓ All packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:59:41.934818Z","iopub.execute_input":"2026-02-13T00:59:41.935539Z","iopub.status.idle":"2026-02-13T01:00:00.116437Z","shell.execute_reply.started":"2026-02-13T00:59:41.935508Z","shell.execute_reply":"2026-02-13T01:00:00.115521Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h✓ All packages installed successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Step 2: Setup Project Structure","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n\n# Create directory structure\ndirectories = [\n    'config',\n    'src',\n    'data/raw',\n    'data/processed',\n    'data/datasets',\n    'models',\n    'outputs',\n    'logs',\n    'cache'\n]\n\nfor directory in directories:\n    os.makedirs(directory, exist_ok=True)\n    \nprint(\"✓ Directory structure created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:00.118264Z","iopub.execute_input":"2026-02-13T01:00:00.118543Z","iopub.status.idle":"2026-02-13T01:00:00.124775Z","shell.execute_reply.started":"2026-02-13T01:00:00.118509Z","shell.execute_reply":"2026-02-13T01:00:00.124161Z"}},"outputs":[{"name":"stdout","text":"✓ Directory structure created!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Step 3: Create Configuration File","metadata":{}},{"cell_type":"code","source":"%%writefile config.yaml\n# Configuration for AI Paper Summarizer & Q&A System\n\nmodel:\n  summarizer:\n    base_model: \"sshleifer/distilbart-cnn-12-6\"\n    max_length: 512\n    min_length: 100\n    num_beams: 4\n    \n  qa:\n    base_model: \"distilbert-base-uncased-distilled-squad\"\n    max_length: 384\n    doc_stride: 128\n    \n  lora:\n    r: 16\n    lora_alpha: 32\n    lora_dropout: 0.1\n    bias: \"none\"\n    task_type: \"SEQ_2_SEQ_LM\"\n\ntraining:\n  summarizer:\n    num_epochs: 3\n    batch_size: 4\n    gradient_accumulation_steps: 4\n    learning_rate: 3e-4\n    warmup_steps: 500\n    weight_decay: 0.01\n    max_grad_norm: 1.0\n    fp16: true\n    \n  qa:\n    num_epochs: 2\n    batch_size: 8\n    gradient_accumulation_steps: 2\n    learning_rate: 5e-5\n    warmup_steps: 300\n    weight_decay: 0.01\n    max_grad_norm: 1.0\n    fp16: true\n\ndata:\n  max_samples: 5000\n  train_split: 0.9\n  val_split: 0.1\n  max_input_length: 1024\n  max_target_length: 256\n  \npaths:\n  data_dir: \"/kaggle/input\"\n  output_dir: \"./outputs\"\n  model_save_dir: \"./models\"\n  logs_dir: \"./logs\"\n  cache_dir: \"./cache\"\n\ndeployment:\n  port: 8000\n  host: \"0.0.0.0\"\n  ngrok_token: \"\"\n  \nevaluation:\n  rouge_types: [\"rouge1\", \"rouge2\", \"rougeL\"]\n  save_predictions: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:00.125963Z","iopub.execute_input":"2026-02-13T01:00:00.126252Z","iopub.status.idle":"2026-02-13T01:00:00.136386Z","shell.execute_reply.started":"2026-02-13T01:00:00.126228Z","shell.execute_reply":"2026-02-13T01:00:00.135711Z"}},"outputs":[{"name":"stdout","text":"Writing config.yaml\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 4: Create Python Modules\n\nCopy the utils.py, data_preparation.py, model_training.py, inference.py, and deployment.py files into the working directory or use the %%writefile magic to create them.","metadata":{}},{"cell_type":"code","source":"# Copy all Python files from input to working directory\n!cp /kaggle/input/allscripts/*.py /kaggle/working/\n!cp /kaggle/input/allscripts/*.yaml /kaggle/working/\n!cp /kaggle/input/allscripts/*.txt /kaggle/working/\n!cp /kaggle/input/allscripts/*.md /kaggle/working/\n\n# Verify files are copied\n!ls -lh /kaggle/working/\n\nprint(\"✓ Files copied successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:00.138175Z","iopub.execute_input":"2026-02-13T01:00:00.138417Z","iopub.status.idle":"2026-02-13T01:00:00.763156Z","shell.execute_reply.started":"2026-02-13T01:00:00.138398Z","shell.execute_reply":"2026-02-13T01:00:00.762432Z"}},"outputs":[{"name":"stdout","text":"total 156K\ndrwxr-xr-x 2 root root 4.0K Feb 13 01:00 cache\n-rw-r--r-- 1 root root  15K Feb 13 01:00 COMPLETE_DOCUMENTATION.md\ndrwxr-xr-x 2 root root 4.0K Feb 13 01:00 config\n-rw-r--r-- 1 root root 1.5K Feb 13 01:00 config.yaml\ndrwxr-xr-x 5 root root 4.0K Feb 13 01:00 data\n-rw-r--r-- 1 root root  15K Feb 13 01:00 data_preparation.py\n-rw-r--r-- 1 root root 8.5K Feb 13 01:00 deployment.py\n-rw-r--r-- 1 root root  13K Feb 13 01:00 inference.py\ndrwxr-xr-x 2 root root 4.0K Feb 13 01:00 logs\ndrwxr-xr-x 2 root root 4.0K Feb 13 01:00 models\n-rw-r--r-- 1 root root  14K Feb 13 01:00 model_training.py\ndrwxr-xr-x 2 root root 4.0K Feb 13 01:00 outputs\n-rw-r--r-- 1 root root 6.4K Feb 13 01:00 QUICKSTART.md\n-rw-r--r-- 1 root root 2.6K Feb 13 01:00 README.md\n-rw-r--r-- 1 root root  502 Feb 13 01:00 requirements.txt\n-rw-r--r-- 1 root root 7.0K Feb 13 01:00 setup.py\ndrwxr-xr-x 2 root root 4.0K Feb 13 01:00 src\n-rw-r--r-- 1 root root 9.8K Feb 13 01:00 test_system.py\n-rw-r--r-- 1 root root 8.8K Feb 13 01:00 utils.py\n✓ Files copied successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Import the modules we'll need\nimport utils\nimport data_preparation\nimport model_training\nimport inference\nimport deployment\n\nprint(\"✓ Modules imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:00.764352Z","iopub.execute_input":"2026-02-13T01:00:00.764595Z","iopub.status.idle":"2026-02-13T01:00:33.880795Z","shell.execute_reply.started":"2026-02-13T01:00:00.764566Z","shell.execute_reply":"2026-02-13T01:00:33.880114Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:00:16.728299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770944416.908114     103 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770944416.960217     103 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770944417.394756     103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770944417.394803     103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770944417.394806     103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770944417.394809     103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"✓ Modules imported successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Step 5: Prepare Data","metadata":{}},{"cell_type":"code","source":"from data_preparation import ArxivDatasetProcessor\nfrom utils import load_config, setup_logging\n\nlogger = setup_logging()\nconfig = load_config()\n\n# Initialize processor\nprocessor = ArxivDatasetProcessor(config)\n\n# Load dataset\nprint(\"Loading arXiv dataset...\")\ndf = processor.load_arxiv_dataset()\nprint(f\"Loaded {len(df)} papers\")\n\n# Preview data\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:33.881584Z","iopub.execute_input":"2026-02-13T01:00:33.881872Z","iopub.status.idle":"2026-02-13T01:00:34.316461Z","shell.execute_reply.started":"2026-02-13T01:00:33.881844Z","shell.execute_reply":"2026-02-13T01:00:34.315788Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:00:33,889 - utils - INFO - Loading arXiv dataset...\n2026-02-13 01:00:33,890 - utils - INFO - Loading from HuggingFace datasets...\n","output_type":"stream"},{"name":"stdout","text":"Loading arXiv dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5084f2a526745fe8e3fa16c9f9042fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scientific_papers.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8f765d3c49546d3ac042edd8ba21ae2"}},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:00:34,291 - utils - ERROR - Error loading dataset: Dataset scripts are no longer supported, but found scientific_papers.py\n2026-02-13 01:00:34,291 - utils - INFO - Creating sample dataset for testing...\n","output_type":"stream"},{"name":"stdout","text":"Loaded 3 papers\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                               title  \\\n0                          Attention Is All You Need   \n1  BERT: Pre-training of Deep Bidirectional Trans...   \n2       GPT-3: Language Models are Few-Shot Learners   \n\n                                            abstract  \\\n0  We propose a new simple network architecture, ...   \n1  We introduce a new language representation mod...   \n2  Recent work has demonstrated substantial gains...   \n\n                                             article  \n0  The dominant sequence transduction models are ...  \n1  Language model pre-training has been shown to ...  \n2  Language models have recently been shown to be...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>article</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Attention Is All You Need</td>\n      <td>We propose a new simple network architecture, ...</td>\n      <td>The dominant sequence transduction models are ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n      <td>We introduce a new language representation mod...</td>\n      <td>Language model pre-training has been shown to ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GPT-3: Language Models are Few-Shot Learners</td>\n      <td>Recent work has demonstrated substantial gains...</td>\n      <td>Language models have recently been shown to be...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Prepare summarization dataset\nprint(\"Preparing summarization dataset...\")\nsumm_dataset = processor.prepare_summarization_data(df)\nsumm_dataset.save_to_disk(\"./data/datasets/summarization\")\n\nprint(f\"Train samples: {len(summ_dataset['train'])}\")\nprint(f\"Validation samples: {len(summ_dataset['validation'])}\")\n\n# Preview sample\nprint(\"\\nSample:\")\nprint(summ_dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:34.317535Z","iopub.execute_input":"2026-02-13T01:00:34.317829Z","iopub.status.idle":"2026-02-13T01:00:34.387498Z","shell.execute_reply.started":"2026-02-13T01:00:34.317800Z","shell.execute_reply":"2026-02-13T01:00:34.386977Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:00:34,319 - utils - INFO - Preparing summarization dataset...\n","output_type":"stream"},{"name":"stdout","text":"Preparing summarization dataset...\n","output_type":"stream"},{"name":"stderr","text":"Processing papers: 100%|██████████| 3/3 [00:00<00:00, 4056.39it/s]\n2026-02-13 01:00:34,346 - utils - INFO - Train samples: 2\n2026-02-13 01:00:34,346 - utils - INFO - Validation samples: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6fbfef49db43d49f8c509b6c74719d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7319bad0ffc4cd087b4a8a509ac40c7"}},"metadata":{}},{"name":"stdout","text":"Train samples: 2\nValidation samples: 1\n\nSample:\n{'article': 'Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.', 'summary': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Prepare Q&A dataset\nprint(\"Preparing Q&A dataset...\")\nqa_dataset = processor.prepare_qa_data(df)\nqa_dataset.save_to_disk(\"./data/datasets/qa\")\n\nprint(f\"Train Q&A pairs: {len(qa_dataset['train'])}\")\nprint(f\"Validation Q&A pairs: {len(qa_dataset['validation'])}\")\n\n# Preview sample\nprint(\"\\nSample:\")\nprint(qa_dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:34.388244Z","iopub.execute_input":"2026-02-13T01:00:34.388534Z","iopub.status.idle":"2026-02-13T01:00:34.448765Z","shell.execute_reply.started":"2026-02-13T01:00:34.388511Z","shell.execute_reply":"2026-02-13T01:00:34.448150Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:00:34,390 - utils - INFO - Preparing Q&A dataset...\n","output_type":"stream"},{"name":"stdout","text":"Preparing Q&A dataset...\n","output_type":"stream"},{"name":"stderr","text":"Generating Q&A pairs: 100%|██████████| 3/3 [00:00<00:00, 4742.90it/s]\n2026-02-13 01:00:34,408 - utils - INFO - Train Q&A pairs: 8\n2026-02-13 01:00:34,409 - utils - INFO - Validation Q&A pairs: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/8 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3463401f349e4859b9266f213cbb4e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52a4883cdc754741916b43b01c29d170"}},"metadata":{}},{"name":"stdout","text":"Train Q&A pairs: 8\nValidation Q&A pairs: 1\n\nSample:\n{'question': 'What is the main contribution of this paper?', 'context': 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.', 'answers': {'answer_start': [155], 'text': ['Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train']}}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Step 6: Train Summarization Model","metadata":{}},{"cell_type":"code","source":"# ===================================\n# CELL: Train Summarization Model\n# ===================================\n\nfrom model_training import SummarizationTrainer\nfrom utils import load_config\nimport yaml\n\nprint(\"=\"*50)\nprint(\"TRAINING SUMMARIZATION MODEL\")\nprint(\"=\"*50)\n\n# Load and fix config\nconfig = load_config()\n\n# Ensure learning rates are floats\nif isinstance(config['training']['summarizer']['learning_rate'], str):\n    config['training']['summarizer']['learning_rate'] = float(config['training']['summarizer']['learning_rate'])\nif isinstance(config['training']['qa']['learning_rate'], str):\n    config['training']['qa']['learning_rate'] = float(config['training']['qa']['learning_rate'])\n\nprint(f\"Learning rate: {config['training']['summarizer']['learning_rate']}\")\n\n# Initialize trainer\nsumm_trainer = SummarizationTrainer(config)\n\n# Setup model\nsumm_trainer.setup_model_and_tokenizer()\n\n# Load data\nsumm_trainer.load_and_tokenize_data()\n\n# Train\nsumm_trainer.train()\n\nprint(\"✓ Summarization model training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:34.449744Z","iopub.execute_input":"2026-02-13T01:00:34.450090Z","iopub.status.idle":"2026-02-13T01:00:56.621559Z","shell.execute_reply.started":"2026-02-13T01:00:34.450051Z","shell.execute_reply":"2026-02-13T01:00:56.620850Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:00:34,462 - utils - INFO - Loading model: sshleifer/distilbart-cnn-12-6\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nTRAINING SUMMARIZATION MODEL\n==================================================\nLearning rate: 0.0003\nUsing GPU: Tesla T4\nGPU Memory: 15.64 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec2f95c02714984b8b43f6c976e039b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da974c9974cb47fd831dda9a846e0d97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543a857705874e939a5d9f7e54e13bb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9088b6f09af14730b69b64084a764cab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1d77ee260943f38abc6d3d0a47b7e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976e197b9af248ca92d15f631483c477"}},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:00:49,457 - utils - INFO - Model memory: 1228.53 MB\n2026-02-13 01:00:49,458 - utils - INFO - Loading summarization dataset...\n2026-02-13 01:00:49,468 - utils - INFO - Tokenizing summarization dataset...\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,572,864 || all params: 307,083,264 || trainable%: 0.5122\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e71a59743b94f2b972bfa3539f4f641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a255c879004940eeb57383e04d347be3"}},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:00:49,581 - utils - INFO - Training samples: 2\n2026-02-13 01:00:49,583 - utils - INFO - Validation samples: 1\n2026-02-13 01:00:49,584 - utils - INFO - Starting summarization model training...\n/kaggle/working/model_training.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b4cda913ed14991a647ec9a2652a7eb"}},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:00:54,726 - utils - INFO - Model saved to ./models/summarizer_final\n2026-02-13 01:00:54,727 - utils - INFO - Training metrics: {'train_runtime': 3.7334, 'train_samples_per_second': 1.607, 'train_steps_per_second': 0.804, 'total_flos': 9345454571520.0, 'train_loss': 12.915903727213541, 'epoch': 3.0}\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:00:56,425 - absl - INFO - Using default tokenizer.\n2026-02-13 01:00:56,428 - utils - INFO - Evaluation results: {'eval_loss': 12.905929565429688, 'eval_rouge1': 0.21153846153846154, 'eval_rouge2': 0.0392156862745098, 'eval_rougeL': 0.1346153846153846, 'eval_runtime': 1.6951, 'eval_samples_per_second': 0.59, 'eval_steps_per_second': 0.59, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stdout","text":"✓ Summarization model training complete!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Step 7: Train Q&A Model","metadata":{}},{"cell_type":"code","source":"from model_training import QATrainer\nfrom utils import clear_cuda_cache\n\n# Clear memory from previous training\ndel summ_trainer\nclear_cuda_cache()\n\nprint(\"=\"*50)\nprint(\"TRAINING Q&A MODEL\")\nprint(\"=\"*50)\n\n# Initialize trainer\nqa_trainer = QATrainer(config)\n\n# Setup model\nqa_trainer.setup_model_and_tokenizer()\n\n# Load data\nqa_trainer.load_and_tokenize_data()\n\n# Train\nqa_trainer.train()\n\nprint(\"✓ Q&A model training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:00:56.625195Z","iopub.execute_input":"2026-02-13T01:00:56.626030Z","iopub.status.idle":"2026-02-13T01:01:01.775741Z","shell.execute_reply.started":"2026-02-13T01:00:56.625991Z","shell.execute_reply":"2026-02-13T01:01:01.774995Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:00:56,668 - utils - INFO - Loading Q&A model: distilbert-base-uncased-distilled-squad\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nTRAINING Q&A MODEL\n==================================================\nUsing GPU: Tesla T4\nGPU Memory: 15.64 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c702c61981f43a0813a4e8674c939e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ed015a86b74de5b21ba1cd9a284617"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c8f3cc87904f9f937eee5809671d1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b06d920a9974eb3848e50fe048bc46a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f473f124fbd242d0a045fddb8b7a614a"}},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:00:59,291 - utils - INFO - Model memory: 265.46 MB\n2026-02-13 01:00:59,292 - utils - INFO - Loading Q&A dataset...\n2026-02-13 01:00:59,300 - utils - INFO - Tokenizing Q&A dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing Q&A:   0%|          | 0/8 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8178ae9978de40818d723679f0232ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing Q&A:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3130df4eb0f4ea0a77b1221e5b1ad57"}},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:00:59,355 - utils - INFO - Training samples: 8\n2026-02-13 01:00:59,356 - utils - INFO - Validation samples: 1\n2026-02-13 01:00:59,357 - utils - INFO - Starting Q&A model training...\n/kaggle/working/model_training.py:318: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:01:01,713 - utils - INFO - Q&A model saved to ./models/qa_final\n2026-02-13 01:01:01,714 - utils - INFO - Training metrics: {'train_runtime': 1.461, 'train_samples_per_second': 10.951, 'train_steps_per_second': 1.369, 'total_flos': 1567837200384.0, 'train_loss': 3.5042238235473633, 'epoch': 2.0}\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:01:01,737 - utils - INFO - Evaluation results: {'eval_loss': 3.149193525314331, 'eval_exact_match': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.0191, 'eval_samples_per_second': 52.377, 'eval_steps_per_second': 52.377, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stdout","text":"✓ Q&A model training complete!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Step 8: Test Models with Sample Paper","metadata":{}},{"cell_type":"code","source":"from inference import PaperQASystem\n\n# Sample paper for testing\nsample_paper = \"\"\"\nAttention Is All You Need\n\nAbstract: We propose a new simple network architecture, the Transformer, \nbased solely on attention mechanisms, dispensing with recurrence and \nconvolutions entirely. Experiments on two machine translation tasks show \nthese models to be superior in quality while being more parallelizable \nand requiring significantly less time to train.\n\nIntroduction: The dominant sequence transduction models are based on \ncomplex recurrent or convolutional neural networks that include an \nencoder and a decoder. The best performing models also connect the \nencoder and decoder through an attention mechanism. We propose a new \nsimple network architecture, the Transformer, based solely on attention \nmechanisms.\n\nThe Transformer follows the overall architecture using stacked self-attention \nand point-wise, fully connected layers for both the encoder and decoder. \nIn this work we propose the Transformer, a model architecture eschewing \nrecurrence and instead relying entirely on an attention mechanism to draw \nglobal dependencies between input and output.\n\nMethods: Our model uses multi-head attention mechanisms which allow the model \nto jointly attend to information from different representation subspaces. \nWe use eight parallel attention layers, or heads. For each of these we use \ndk=dv=dmodel/h=64 dimensions.\n\"\"\"\n\nprint(\"Loading trained models...\")\nsystem = PaperQASystem(\n    summarizer_path=\"./models/summarizer_final\",\n    qa_path=\"./models/qa_final\"\n)\n\nprint(\"✓ Models loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:01.776738Z","iopub.execute_input":"2026-02-13T01:01:01.777086Z","iopub.status.idle":"2026-02-13T01:01:07.627462Z","shell.execute_reply.started":"2026-02-13T01:01:01.777046Z","shell.execute_reply":"2026-02-13T01:01:07.625995Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:01:01,784 - utils - INFO - Initializing Paper Q&A System...\n2026-02-13 01:01:01,791 - utils - INFO - Loading summarization model from ./models/summarizer_final\n","output_type":"stream"},{"name":"stdout","text":"Loading trained models...\nUsing GPU: Tesla T4\nGPU Memory: 15.64 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a11ae289524b40d0be26a14c92e9d3d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e3951879ac44bea9d6b02e50b6ee647"}},"metadata":{}},{"name":"stderr","text":"2026-02-13 01:01:07,209 - utils - INFO - Summarization model loaded successfully\n2026-02-13 01:01:07,229 - utils - INFO - Loading Q&A model from ./models/qa_final\n","output_type":"stream"},{"name":"stdout","text":"Using GPU: Tesla T4\nGPU Memory: 15.64 GB\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n2026-02-13 01:01:07,617 - utils - INFO - Q&A model loaded successfully\n2026-02-13 01:01:07,621 - utils - INFO - System initialized successfully\n","output_type":"stream"},{"name":"stdout","text":"✓ Models loaded successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Test summarization\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING SUMMARIZATION\")\nprint(\"=\"*60)\n\nsummary = system.summarizer.summarize(sample_paper)\n\nprint(f\"\\nOriginal length: {len(sample_paper.split())} words\")\nprint(f\"Summary length: {len(summary.split())} words\")\nprint(f\"\\nSUMMARY:\\n{summary}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:07.631323Z","iopub.execute_input":"2026-02-13T01:01:07.632813Z","iopub.status.idle":"2026-02-13T01:01:11.285056Z","shell.execute_reply.started":"2026-02-13T01:01:07.632755Z","shell.execute_reply":"2026-02-13T01:01:11.284145Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTESTING SUMMARIZATION\n============================================================\n\nOriginal length: 185 words\nSummary length: 86 words\n\nSUMMARY:\n We propose a new simple network architecture, the Transformer, based solely on attention mechanisms. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. The Transformer follows the overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Our model uses multi-head attention mechanisms which allow the model to jointly attend to information from different representation subspaces. For each of these we use dk=dv=dmodel/h=64 dimensions.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Test Q&A\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING Q&A\")\nprint(\"=\"*60)\n\ntest_questions = [\n    \"What is the main contribution?\",\n    \"What architecture is proposed?\",\n    \"How many attention heads are used?\",\n    \"What does the Transformer replace?\"\n]\n\nfor question in test_questions:\n    result = system.qa.answer_question(question, sample_paper)\n    print(f\"\\nQ: {question}\")\n    print(f\"A: {result['answer']}\")\n    print(f\"Confidence: {result['score']:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:11.286071Z","iopub.execute_input":"2026-02-13T01:01:11.286378Z","iopub.status.idle":"2026-02-13T01:01:11.343276Z","shell.execute_reply.started":"2026-02-13T01:01:11.286350Z","shell.execute_reply":"2026-02-13T01:01:11.342543Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTESTING Q&A\n============================================================\n\nQ: What is the main contribution?\nA: draw global dependencies between input and output\nConfidence: 13.59%\n\nQ: What architecture is proposed?\nA: the Transformer\nConfidence: 28.04%\n\nQ: How many attention heads are used?\nA: eight\nConfidence: 91.05%\n\nQ: What does the Transformer replace?\nA: an attention mechanism to draw global dependencies between input and output\nConfidence: 9.93%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Test complete pipeline\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING COMPLETE PIPELINE\")\nprint(\"=\"*60)\n\nresults = system.process_paper(\n    paper_text=sample_paper,\n    questions=test_questions\n)\n\nprint(f\"\\nSUMMARY:\\n{results['summary']}\")\nprint(\"\\nQ&A RESULTS:\")\nfor qa in results['qa_results']:\n    print(f\"\\nQ: {qa['question']}\")\n    print(f\"A: {qa['answer']}\")\n    print(f\"Confidence: {qa['score']:.2%}\")\n\nprint(f\"\\nTotal processing time: {results['processing_time']:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:11.344269Z","iopub.execute_input":"2026-02-13T01:01:11.344668Z","iopub.status.idle":"2026-02-13T01:01:23.056267Z","shell.execute_reply.started":"2026-02-13T01:01:11.344639Z","shell.execute_reply":"2026-02-13T01:01:23.055443Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:01:21,375 - utils - INFO - Generating summary...\n2026-02-13 01:01:21,376 - utils - INFO - Processing 1 chunks...\n2026-02-13 01:01:21,377 - utils - INFO - Summarizing chunk 1/1\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nTESTING COMPLETE PIPELINE\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"2026-02-13 01:01:23,010 - utils - INFO - Answering 4 questions...\n","output_type":"stream"},{"name":"stdout","text":"\nSUMMARY:\n We propose a new simple network architecture, the Transformer, based solely on attention mechanisms. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. The Transformer follows the overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Our model uses multi-head attention mechanisms which allow the model to jointly attend to information from different representation subspaces. For each of these we use dk=dv=dmodel/h=64 dimensions.\n\nQ&A RESULTS:\n\nQ: What is the main contribution?\nA: draw global dependencies between input and output\nConfidence: 13.59%\n\nQ: What architecture is proposed?\nA: the Transformer\nConfidence: 28.04%\n\nQ: How many attention heads are used?\nA: eight\nConfidence: 91.05%\n\nQ: What does the Transformer replace?\nA: an attention mechanism to draw global dependencies between input and output\nConfidence: 9.93%\n\nTotal processing time: 1.68s\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Step 9: Deploy with FastAPI and Ngrok","metadata":{}},{"cell_type":"code","source":"\nNGROK_TOKEN = \"2sV7t8ID8Ezdy2IJEB3yngswQf7_81vvYkxHYuNe7z6AYBC2A\"  \n\n# Update config with ngrok token\nif NGROK_TOKEN:\n    config['deployment']['ngrok_token'] = NGROK_TOKEN\n    import yaml\n    with open('config.yaml', 'w') as f:\n        yaml.dump(config, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:23.057344Z","iopub.execute_input":"2026-02-13T01:01:23.057717Z","iopub.status.idle":"2026-02-13T01:01:23.065288Z","shell.execute_reply.started":"2026-02-13T01:01:23.057680Z","shell.execute_reply":"2026-02-13T01:01:23.064419Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nos.makedirs('static', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:23.066251Z","iopub.execute_input":"2026-02-13T01:01:23.066594Z","iopub.status.idle":"2026-02-13T01:01:23.077844Z","shell.execute_reply.started":"2026-02-13T01:01:23.066558Z","shell.execute_reply":"2026-02-13T01:01:23.077173Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ===================================\n# CELL: HTML CODE\n# ===================================\n\nimport os\n\nhtml_better_errors = '''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI Paper Analyzer</title>\n    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap\" rel=\"stylesheet\">\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n    <style>\n        * { margin: 0; padding: 0; box-sizing: border-box; }\n        body {\n            font-family: 'Inter', sans-serif;\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            min-height: 100vh;\n            padding: 20px;\n            color: #333;\n        }\n        .container { max-width: 1400px; margin: 0 auto; }\n        .header {\n            text-align: center;\n            color: white;\n            margin-bottom: 40px;\n            animation: fadeInDown 0.8s ease;\n        }\n        .app-title {\n            font-size: 3.5rem;\n            font-weight: 700;\n            margin-bottom: 10px;\n            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);\n        }\n        .app-subtitle {\n            font-size: 1.3rem;\n            font-weight: 300;\n            opacity: 0.95;\n        }\n        .main-content {\n            display: grid;\n            grid-template-columns: 1fr 1fr;\n            gap: 30px;\n            animation: fadeInUp 0.8s ease;\n        }\n        .panel {\n            background: white;\n            border-radius: 20px;\n            padding: 35px;\n            box-shadow: 0 20px 60px rgba(0,0,0,0.3);\n            transition: transform 0.3s ease;\n        }\n        .panel:hover { transform: translateY(-5px); }\n        .panel-title {\n            font-size: 1.8rem;\n            font-weight: 600;\n            color: #667eea;\n            margin-bottom: 25px;\n            display: flex;\n            align-items: center;\n            gap: 12px;\n        }\n        .upload-area {\n            border: 3px dashed #667eea;\n            border-radius: 15px;\n            padding: 50px 30px;\n            text-align: center;\n            cursor: pointer;\n            transition: all 0.3s ease;\n            background: #f8f9ff;\n            margin-bottom: 25px;\n        }\n        .upload-area:hover { background: #f0f2ff; border-color: #764ba2; }\n        .upload-icon { font-size: 3.5rem; color: #667eea; margin-bottom: 15px; }\n        .upload-text { font-size: 1.1rem; color: #666; margin-bottom: 8px; }\n        .upload-hint { font-size: 0.9rem; color: #999; }\n        .file-input { display: none; }\n        .file-name {\n            display: none;\n            margin-top: 15px;\n            padding: 12px;\n            background: #e8f5e9;\n            border-radius: 8px;\n            color: #2e7d32;\n            font-weight: 500;\n        }\n        .mode-selection {\n            display: grid;\n            grid-template-columns: 1fr 1fr;\n            gap: 15px;\n            margin-bottom: 25px;\n        }\n        .mode-btn {\n            padding: 15px;\n            border: 2px solid #e0e0e0;\n            border-radius: 12px;\n            background: white;\n            cursor: pointer;\n            transition: all 0.3s ease;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            gap: 10px;\n            font-size: 1rem;\n            font-weight: 600;\n            color: #666;\n        }\n        .mode-btn:hover { border-color: #667eea; background: #f8f9ff; }\n        .mode-btn.active {\n            border-color: #667eea;\n            background: linear-gradient(135deg, #f8f9ff 0%, #f0f2ff 100%);\n            color: #667eea;\n            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.2);\n        }\n        .questions-section { display: none; }\n        .questions-section.active { display: block; }\n        .question-item { display: flex; gap: 10px; margin-bottom: 12px; }\n        .question-input {\n            flex: 1;\n            padding: 14px 18px;\n            border: 2px solid #e0e0e0;\n            border-radius: 10px;\n            font-size: 1rem;\n            font-family: 'Inter', sans-serif;\n        }\n        .question-input:focus {\n            outline: none;\n            border-color: #667eea;\n            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);\n        }\n        .remove-question {\n            background: #ff5252;\n            color: white;\n            border: none;\n            width: 45px;\n            border-radius: 10px;\n            cursor: pointer;\n            font-size: 1.2rem;\n        }\n        .add-question {\n            width: 100%;\n            padding: 14px;\n            background: #f0f2ff;\n            border: 2px dashed #667eea;\n            border-radius: 10px;\n            color: #667eea;\n            font-weight: 600;\n            cursor: pointer;\n            font-size: 1rem;\n        }\n        .action-buttons {\n            display: grid;\n            grid-template-columns: 1fr 2fr;\n            gap: 15px;\n            margin-top: 25px;\n        }\n        .btn {\n            padding: 16px 32px;\n            border: none;\n            border-radius: 12px;\n            font-size: 1.1rem;\n            font-weight: 600;\n            cursor: pointer;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            gap: 10px;\n            transition: all 0.3s ease;\n        }\n        .btn-primary {\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            color: white;\n            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.4);\n        }\n        .btn-primary:hover {\n            transform: translateY(-2px);\n            box-shadow: 0 15px 40px rgba(102, 126, 234, 0.5);\n        }\n        .btn-primary:disabled { opacity: 0.5; cursor: not-allowed; }\n        .btn-secondary { background: white; color: #667eea; border: 2px solid #667eea; }\n        .btn-secondary:hover { background: #f0f2ff; }\n        .results-container { display: none; }\n        .summary-box {\n            background: linear-gradient(135deg, #f8f9ff 0%, #f0f2ff 100%);\n            padding: 25px;\n            border-radius: 15px;\n            border-left: 5px solid #667eea;\n            line-height: 1.8;\n            font-size: 1.05rem;\n        }\n        .answer-card {\n            background: white;\n            border: 2px solid #e0e0e0;\n            border-radius: 15px;\n            padding: 20px;\n            margin-bottom: 15px;\n            transition: all 0.3s ease;\n        }\n        .answer-card:hover {\n            border-color: #667eea;\n            box-shadow: 0 5px 20px rgba(102, 126, 234, 0.15);\n        }\n        .answer-question {\n            font-weight: 600;\n            color: #667eea;\n            font-size: 1.1rem;\n            margin-bottom: 12px;\n        }\n        .answer-text {\n            color: #333;\n            line-height: 1.7;\n            margin-bottom: 12px;\n            padding-left: 30px;\n        }\n        .confidence-bar {\n            display: flex;\n            align-items: center;\n            gap: 12px;\n            padding-left: 30px;\n        }\n        .progress-bar {\n            flex: 1;\n            height: 10px;\n            background: #e0e0e0;\n            border-radius: 10px;\n            overflow: hidden;\n        }\n        .progress-fill {\n            height: 100%;\n            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n        }\n        .loading { display: none; text-align: center; padding: 40px; }\n        .spinner {\n            border: 4px solid #f3f3f3;\n            border-top: 4px solid #667eea;\n            border-radius: 50%;\n            width: 50px;\n            height: 50px;\n            animation: spin 1s linear infinite;\n            margin: 0 auto 20px;\n        }\n        .error-display {\n            background: #ffebee;\n            border: 2px solid #f44336;\n            border-radius: 10px;\n            padding: 20px;\n            margin: 20px 0;\n            color: #c62828;\n        }\n        .error-display h4 {\n            margin-bottom: 10px;\n            display: flex;\n            align-items: center;\n            gap: 10px;\n        }\n        @keyframes spin { 100% { transform: rotate(360deg); } }\n        @keyframes fadeInDown { from { opacity: 0; transform: translateY(-30px); } to { opacity: 1; transform: translateY(0); } }\n        @keyframes fadeInUp { from { opacity: 0; transform: translateY(30px); } to { opacity: 1; transform: translateY(0); } }\n        @media (max-width: 968px) {\n            .main-content { grid-template-columns: 1fr; }\n            .app-title { font-size: 2.5rem; }\n            .mode-selection { grid-template-columns: 1fr; }\n        }\n        .toast {\n            position: fixed;\n            bottom: 30px;\n            right: 30px;\n            background: white;\n            padding: 20px 25px;\n            border-radius: 12px;\n            box-shadow: 0 10px 40px rgba(0,0,0,0.3);\n            display: none;\n            z-index: 1000;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"header\">\n            <h1 class=\"app-title\"><i class=\"fas fa-brain\"></i> AI Paper Analyzer</h1>\n            <p class=\"app-subtitle\">Professional Research Assistant powered by Advanced AI</p>\n        </div>\n        <div class=\"main-content\">\n            <div class=\"panel\">\n                <h2 class=\"panel-title\"><i class=\"fas fa-upload\"></i> Upload Paper</h2>\n                <div class=\"upload-area\" id=\"uploadArea\">\n                    <div class=\"upload-icon\"><i class=\"fas fa-cloud-upload-alt\"></i></div>\n                    <div class=\"upload-text\">Drag & Drop your paper here</div>\n                    <div class=\"upload-hint\">or click to browse (PDF, TXT)</div>\n                    <input type=\"file\" id=\"fileInput\" class=\"file-input\" accept=\".pdf,.txt\">\n                </div>\n                <div class=\"file-name\" id=\"fileName\"></div>\n                <h2 class=\"panel-title\" style=\"margin-top: 30px;\"><i class=\"fas fa-tasks\"></i> Select Mode</h2>\n                <div class=\"mode-selection\">\n                    <button class=\"mode-btn\" id=\"summarizeBtn\" onclick=\"selectMode('summarize')\">\n                        <i class=\"fas fa-file-alt\"></i> Summarize Only\n                    </button>\n                    <button class=\"mode-btn\" id=\"qaBtn\" onclick=\"selectMode('qa')\">\n                        <i class=\"fas fa-question-circle\"></i> Q&A Only\n                    </button>\n                </div>\n                <div class=\"questions-section\" id=\"questionsSection\">\n                    <h2 class=\"panel-title\"><i class=\"fas fa-comments\"></i> Your Questions</h2>\n                    <div id=\"questionsContainer\">\n                        <div class=\"question-item\">\n                            <input type=\"text\" class=\"question-input\" placeholder=\"Enter your question...\">\n                            <button class=\"remove-question\" onclick=\"removeQuestion(this)\"><i class=\"fas fa-times\"></i></button>\n                        </div>\n                    </div>\n                    <button class=\"add-question\" onclick=\"addQuestion()\"><i class=\"fas fa-plus\"></i> Add Another Question</button>\n                </div>\n                <div class=\"action-buttons\">\n                    <button class=\"btn btn-secondary\" onclick=\"clearAll()\"><i class=\"fas fa-redo\"></i> Reset</button>\n                    <button class=\"btn btn-primary\" id=\"processBtn\" onclick=\"processDocument()\"><i class=\"fas fa-magic\"></i> Process</button>\n                </div>\n            </div>\n            <div class=\"panel\">\n                <h2 class=\"panel-title\"><i class=\"fas fa-chart-line\"></i> Results</h2>\n                <div class=\"loading\" id=\"loadingState\">\n                    <div class=\"spinner\"></div>\n                    <div>Processing your paper...</div>\n                </div>\n                <div class=\"results-container\" id=\"resultsContainer\">\n                    <div id=\"summarySection\" style=\"display: none;\">\n                        <h3 style=\"color: #667eea; margin-bottom: 15px;\"><i class=\"fas fa-file-alt\"></i> Summary</h3>\n                        <div class=\"summary-box\" id=\"summaryBox\"></div>\n                    </div>\n                    <div id=\"qaSection\" style=\"display: none;\">\n                        <h3 style=\"color: #667eea; margin-bottom: 15px;\"><i class=\"fas fa-comments\"></i> Answers</h3>\n                        <div id=\"answersContainer\"></div>\n                    </div>\n                    <div id=\"processingTime\" style=\"text-align: center; margin-top: 20px; color: #667eea;\"></div>\n                </div>\n                <div id=\"errorDisplay\" style=\"display: none;\"></div>\n                <div id=\"initialState\" style=\"text-align: center; color: #999; padding: 60px 20px;\">\n                    <i class=\"fas fa-lightbulb\" style=\"font-size: 4rem; opacity: 0.3; margin-bottom: 20px;\"></i>\n                    <p style=\"font-size: 1.2rem;\">Upload a paper and select a mode to get started</p>\n                    <p style=\"font-size: 0.9rem; margin-top: 10px; opacity: 0.7;\">Choose \"Summarize\" for quick overview or \"Q&A\" to ask specific questions</p>\n                </div>\n            </div>\n        </div>\n    </div>\n    <div class=\"toast\" id=\"toast\"></div>\n    <script>\n        const API_BASE_URL = window.location.origin;\n        let uploadedFile = null;\n        let selectedMode = null;\n\n        document.getElementById('uploadArea').addEventListener('click', () => document.getElementById('fileInput').click());\n        document.getElementById('fileInput').addEventListener('change', (e) => {\n            const file = e.target.files[0];\n            if (file && (file.type === 'application/pdf' || file.type === 'text/plain' || file.name.endsWith('.pdf') || file.name.endsWith('.txt'))) {\n                uploadedFile = file;\n                document.getElementById('fileName').textContent = `✓ ${file.name} (${(file.size / 1024).toFixed(1)} KB)`;\n                document.getElementById('fileName').style.display = 'block';\n                showToast('File uploaded successfully!', 'success');\n            } else {\n                showToast('Please upload PDF or TXT file', 'error');\n            }\n        });\n\n        function selectMode(mode) {\n            selectedMode = mode;\n            document.getElementById('summarizeBtn').classList.remove('active');\n            document.getElementById('qaBtn').classList.remove('active');\n            if (mode === 'summarize') {\n                document.getElementById('summarizeBtn').classList.add('active');\n                document.getElementById('questionsSection').classList.remove('active');\n            } else if (mode === 'qa') {\n                document.getElementById('qaBtn').classList.add('active');\n                document.getElementById('questionsSection').classList.add('active');\n            }\n        }\n\n        function addQuestion() {\n            const container = document.getElementById('questionsContainer');\n            const div = document.createElement('div');\n            div.className = 'question-item';\n            div.innerHTML = `\n                <input type=\"text\" class=\"question-input\" placeholder=\"Enter your question...\">\n                <button class=\"remove-question\" onclick=\"removeQuestion(this)\"><i class=\"fas fa-times\"></i></button>\n            `;\n            container.appendChild(div);\n        }\n\n        function removeQuestion(btn) {\n            if (document.getElementById('questionsContainer').children.length > 1) {\n                btn.parentElement.remove();\n            }\n        }\n\n        function clearAll() {\n            uploadedFile = null;\n            selectedMode = null;\n            document.getElementById('fileInput').value = '';\n            document.getElementById('fileName').style.display = 'none';\n            document.getElementById('summarizeBtn').classList.remove('active');\n            document.getElementById('qaBtn').classList.remove('active');\n            document.getElementById('questionsSection').classList.remove('active');\n            document.getElementById('questionsContainer').innerHTML = `\n                <div class=\"question-item\">\n                    <input type=\"text\" class=\"question-input\" placeholder=\"Enter your question...\">\n                    <button class=\"remove-question\" onclick=\"removeQuestion(this)\"><i class=\"fas fa-times\"></i></button>\n                </div>\n            `;\n            document.getElementById('resultsContainer').style.display = 'none';\n            document.getElementById('errorDisplay').style.display = 'none';\n            document.getElementById('initialState').style.display = 'block';\n            showToast('Form reset', 'success');\n        }\n\n        async function processDocument() {\n            if (!uploadedFile) {\n                showToast('Please upload a file first', 'error');\n                return;\n            }\n            if (!selectedMode) {\n                showToast('Please select a mode (Summarize or Q&A)', 'error');\n                return;\n            }\n\n            const inputs = document.querySelectorAll('.question-input');\n            const questions = Array.from(inputs).map(i => i.value.trim()).filter(q => q);\n            \n            if (selectedMode === 'qa' && questions.length === 0) {\n                showToast('Please enter at least one question for Q&A mode', 'error');\n                return;\n            }\n\n            document.getElementById('loadingState').style.display = 'block';\n            document.getElementById('resultsContainer').style.display = 'none';\n            document.getElementById('errorDisplay').style.display = 'none';\n            document.getElementById('initialState').style.display = 'none';\n            document.getElementById('processBtn').disabled = true;\n\n            try {\n                const formData = new FormData();\n                formData.append('file', uploadedFile);\n                formData.append('questions', selectedMode === 'qa' ? questions.join(',') : '');\n\n                console.log('=== REQUEST ===');\n                console.log('File:', uploadedFile.name, uploadedFile.type);\n                console.log('Mode:', selectedMode);\n                console.log('Questions:', questions);\n\n                const response = await fetch(`${API_BASE_URL}/upload-pdf`, {\n                    method: 'POST',\n                    body: formData\n                });\n\n                console.log('=== RESPONSE ===');\n                console.log('Status:', response.status);\n                console.log('Status Text:', response.statusText);\n                \n                const responseText = await response.text();\n                console.log('Raw Response:', responseText);\n                \n                let data;\n                try {\n                    data = JSON.parse(responseText);\n                } catch (e) {\n                    throw new Error(`Invalid JSON response: ${responseText}`);\n                }\n                \n                console.log('Parsed Data:', data);\n\n                if (!response.ok) {\n                    throw new Error(data.detail || 'Processing failed');\n                }\n                \n                document.getElementById('summarySection').style.display = 'none';\n                document.getElementById('qaSection').style.display = 'none';\n                \n                if (selectedMode === 'summarize') {\n                    if (data.summary && data.summary.trim()) {\n                        document.getElementById('summaryBox').textContent = data.summary;\n                        document.getElementById('summarySection').style.display = 'block';\n                    } else {\n                        throw new Error('No summary received from server');\n                    }\n                } else if (selectedMode === 'qa') {\n                    if (data.qa_results && Array.isArray(data.qa_results) && data.qa_results.length > 0) {\n                        const answersContainer = document.getElementById('answersContainer');\n                        answersContainer.innerHTML = '';\n                        \n                        data.qa_results.forEach(qa => {\n                            const card = document.createElement('div');\n                            card.className = 'answer-card';\n                            const confidence = (qa.score * 100).toFixed(1);\n                            card.innerHTML = `\n                                <div class=\"answer-question\"><i class=\"fas fa-question-circle\"></i> ${qa.question}</div>\n                                <div class=\"answer-text\"><i class=\"fas fa-comment-dots\" style=\"color: #667eea;\"></i> ${qa.answer}</div>\n                                <div class=\"confidence-bar\">\n                                    <span style=\"color: #666; font-size: 0.9rem;\">Confidence:</span>\n                                    <div class=\"progress-bar\">\n                                        <div class=\"progress-fill\" style=\"width: ${confidence}%\"></div>\n                                    </div>\n                                    <span style=\"color: #667eea; font-weight: 600;\">${confidence}%</span>\n                                </div>\n                            `;\n                            answersContainer.appendChild(card);\n                        });\n                        \n                        document.getElementById('qaSection').style.display = 'block';\n                    } else {\n                        throw new Error('No Q&A results received from server');\n                    }\n                }\n\n                document.getElementById('processingTime').innerHTML = \n                    `<i class=\"fas fa-clock\"></i> Processing completed in ${data.processing_time.toFixed(2)} seconds`;\n                \n                document.getElementById('resultsContainer').style.display = 'block';\n                showToast('Processing complete!', 'success');\n\n            } catch (error) {\n                console.error('=== ERROR ===');\n                console.error(error);\n                \n                const errorDiv = document.getElementById('errorDisplay');\n                errorDiv.className = 'error-display';\n                errorDiv.innerHTML = `\n                    <h4><i class=\"fas fa-exclamation-triangle\"></i> Error Processing Document</h4>\n                    <p><strong>Error:</strong> ${error.message}</p>\n                    <p><strong>File:</strong> ${uploadedFile.name}</p>\n                    <p><strong>Mode:</strong> ${selectedMode}</p>\n                    <p style=\"margin-top: 10px; font-size: 0.9rem;\">Check the browser console (F12) for detailed logs.</p>\n                `;\n                errorDiv.style.display = 'block';\n                \n                showToast(error.message, 'error');\n            } finally {\n                document.getElementById('loadingState').style.display = 'none';\n                document.getElementById('processBtn').disabled = false;\n            }\n        }\n\n        function showToast(message, type) {\n            const toast = document.getElementById('toast');\n            toast.textContent = message;\n            toast.style.display = 'block';\n            toast.style.borderLeft = type === 'success' ? '5px solid #4caf50' : '5px solid #f44336';\n            setTimeout(() => toast.style.display = 'none', 3000);\n        }\n    </script>\n</body>\n</html>'''\n\nwith open('static/index.html', 'w', encoding='utf-8') as f:\n    f.write(html_better_errors)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:23.078972Z","iopub.execute_input":"2026-02-13T01:01:23.079177Z","iopub.status.idle":"2026-02-13T01:01:23.097042Z","shell.execute_reply.started":"2026-02-13T01:01:23.079145Z","shell.execute_reply":"2026-02-13T01:01:23.096305Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ===================================\n# CELL: Create Bulletproof Backend\n# ===================================\n\ndeployment_fixed = '''import os\nfrom fastapi import FastAPI, HTTPException, UploadFile, File, Form\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel\nfrom typing import Optional, List\nimport PyPDF2\nfrom io import BytesIO\nimport time\nimport traceback\n\nfrom inference import PaperQASystem\nfrom utils import setup_logging, load_config\n\nlogger = setup_logging()\n\nclass SummarizeRequest(BaseModel):\n    text: str\n    max_length: Optional[int] = None\n    min_length: Optional[int] = None\n\nclass QuestionRequest(BaseModel):\n    question: str\n    context: str\n\nclass MultiQuestionRequest(BaseModel):\n    questions: List[str]\n    context: str\n\napp = FastAPI(\n    title=\"AI Paper Analyzer\",\n    description=\"Professional AI Research Assistant\",\n    version=\"1.0.0\"\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nos.makedirs(\"static\", exist_ok=True)\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\nsystem = None\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    global system\n    logger.info(\"Loading models...\")\n    try:\n        system = PaperQASystem(\n            summarizer_path=\"./models/summarizer_final\",\n            qa_path=\"./models/qa_final\"\n        )\n        logger.info(\"Models loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load models: {e}\")\n        try:\n            system = PaperQASystem(\n                summarizer_path=\"sshleifer/distilbart-cnn-12-6\",\n                qa_path=\"distilbert-base-uncased-distilled-squad\"\n            )\n        except Exception as e2:\n            logger.error(f\"Failed to load base models: {e2}\")\n            raise\n\n@app.get(\"/\")\nasync def root():\n    return FileResponse(\"static/index.html\")\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"models_loaded\": system is not None}\n\n@app.post(\"/summarize\")\nasync def summarize_text(request: SummarizeRequest):\n    try:\n        if not system:\n            raise HTTPException(status_code=503, detail=\"Models not loaded\")\n        \n        summary = system.summarizer.summarize(\n            text=request.text,\n            max_length=request.max_length,\n            min_length=request.min_length\n        )\n        \n        return {\n            \"summary\": summary,\n            \"input_length\": len(request.text.split()),\n            \"summary_length\": len(summary.split())\n        }\n    except Exception as e:\n        logger.error(f\"Summarization error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/ask\")\nasync def answer_question(request: QuestionRequest):\n    try:\n        if not system:\n            raise HTTPException(status_code=503, detail=\"Models not loaded\")\n        \n        result = system.qa.answer_question(\n            question=request.question,\n            context=request.context\n        )\n        return result\n    except Exception as e:\n        logger.error(f\"Q&A error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/upload-pdf\")\nasync def upload_file(\n    file: UploadFile = File(...),\n    questions: str = Form(default=\"\")\n):\n    \"\"\"Upload and process a file (PDF or TXT)\"\"\"\n    try:\n        if not system:\n            raise HTTPException(status_code=503, detail=\"Models not loaded\")\n        \n        logger.info(f\"Received file: {file.filename}, content_type: {file.content_type}\")\n        logger.info(f\"Questions parameter: {questions}\")\n        \n        # Read file contents\n        contents = await file.read()\n        logger.info(f\"Read {len(contents)} bytes\")\n        \n        # Extract text based on file type\n        text = \"\"\n        filename_lower = file.filename.lower()\n        \n        try:\n            if filename_lower.endswith('.pdf'):\n                logger.info(\"Processing as PDF...\")\n                pdf_file = BytesIO(contents)\n                pdf_reader = PyPDF2.PdfReader(pdf_file)\n                logger.info(f\"PDF has {len(pdf_reader.pages)} pages\")\n                \n                for i, page in enumerate(pdf_reader.pages):\n                    page_text = page.extract_text()\n                    text += page_text + \"\\\\n\"\n                    logger.info(f\"Page {i+1}: extracted {len(page_text)} chars\")\n                    \n            elif filename_lower.endswith('.txt'):\n                logger.info(\"Processing as TXT...\")\n                try:\n                    text = contents.decode('utf-8')\n                except UnicodeDecodeError:\n                    text = contents.decode('latin-1')\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type: {file.filename}. Please upload PDF or TXT.\"\n                )\n            \n            logger.info(f\"Total extracted text: {len(text)} characters\")\n            \n            if not text.strip():\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"No text could be extracted from the file\"\n                )\n        \n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Text extraction failed: {e}\")\n            logger.error(traceback.format_exc())\n            raise HTTPException(\n                status_code=500,\n                detail=f\"Failed to extract text: {str(e)}\"\n            )\n        \n        # Process based on whether questions were provided\n        start_time = time.time()\n        \n        if questions and questions.strip():\n            # Q&A Mode\n            question_list = [q.strip() for q in questions.split(',') if q.strip()]\n            logger.info(f\"Q&A mode: processing {len(question_list)} questions\")\n            \n            try:\n                qa_results = system.qa.answer_multiple_questions(question_list, text)\n                processing_time = time.time() - start_time\n                \n                logger.info(f\"Q&A completed: {len(qa_results)} answers in {processing_time:.2f}s\")\n                \n                return {\n                    \"summary\": \"\",\n                    \"qa_results\": qa_results,\n                    \"processing_time\": processing_time,\n                    \"filename\": file.filename,\n                    \"text_length\": len(text.split()),\n                    \"mode\": \"qa\"\n                }\n            except Exception as e:\n                logger.error(f\"Q&A processing failed: {e}\")\n                logger.error(traceback.format_exc())\n                raise HTTPException(status_code=500, detail=f\"Q&A failed: {str(e)}\")\n        else:\n            # Summarize Mode\n            logger.info(\"Summarize mode: generating summary\")\n            \n            try:\n                summary = system.summarizer.summarize(text)\n                processing_time = time.time() - start_time\n                \n                logger.info(f\"Summary completed in {processing_time:.2f}s\")\n                \n                return {\n                    \"summary\": summary,\n                    \"qa_results\": [],\n                    \"processing_time\": processing_time,\n                    \"filename\": file.filename,\n                    \"text_length\": len(text.split()),\n                    \"mode\": \"summarize\"\n                }\n            except Exception as e:\n                logger.error(f\"Summarization failed: {e}\")\n                logger.error(traceback.format_exc())\n                raise HTTPException(status_code=500, detail=f\"Summarization failed: {str(e)}\")\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        logger.error(traceback.format_exc())\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n'''\n\nwith open('deployment_ui.py', 'w') as f:\n    f.write(deployment_fixed)\n\nprint(\"✓ Created bulletproof backend with:\")\nprint(\"  - Extensive logging at every step\")\nprint(\"  - Better error handling\")\nprint(\"  - Form data instead of query params\")\nprint(\"  - Detailed error messages\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:23.097839Z","iopub.execute_input":"2026-02-13T01:01:23.098086Z","iopub.status.idle":"2026-02-13T01:01:23.109870Z","shell.execute_reply.started":"2026-02-13T01:01:23.098064Z","shell.execute_reply":"2026-02-13T01:01:23.109271Z"}},"outputs":[{"name":"stdout","text":"✓ Created bulletproof backend with:\n  - Extensive logging at every step\n  - Better error handling\n  - Form data instead of query params\n  - Detailed error messages\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Restart server\nimport nest_asyncio\nnest_asyncio.apply()\n\nimport threading, time\nfrom pyngrok import ngrok\nimport uvicorn\nfrom deployment_ui import app\n\nPORT = 8000\nNGROK_TOKEN = \"2sV7t8ID8Ezdy2IJEB3yngswQf7_81vvYkxHYuNe7z6AYBC2A\"\n\nngrok.set_auth_token(NGROK_TOKEN) if NGROK_TOKEN != \"YOUR_TOKEN\" else None\n\nclass ServerRunner:\n    def __init__(self, app, port):\n        self.app, self.port = app, port\n    def run(self):\n        uvicorn.Server(uvicorn.Config(self.app, host=\"0.0.0.0\", port=self.port, log_level=\"info\")).run()\n\nthreading.Thread(target=ServerRunner(app, PORT).run, daemon=True).start()\ntime.sleep(5)\n\npublic_url = ngrok.connect(PORT, bind_tls=True)\nprint(f\"\\n✅ LIVE AT: {public_url}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:23.110687Z","iopub.execute_input":"2026-02-13T01:01:23.110995Z","iopub.status.idle":"2026-02-13T01:01:29.332538Z","shell.execute_reply.started":"2026-02-13T01:01:23.110963Z","shell.execute_reply":"2026-02-13T01:01:29.331817Z"}},"outputs":[{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"},{"name":"stderr","text":"2026-02-13 01:01:24,071 - pyngrok.process - INFO - Updating authtoken for default \"config_path\" of \"ngrok_path\": /root/.config/ngrok/ngrok\nINFO:     Started server process [103]\nINFO:     Waiting for application startup.\n2026-02-13 01:01:24,232 - utils - INFO - Loading models...\n2026-02-13 01:01:24,237 - utils - INFO - Initializing Paper Q&A System...\n2026-02-13 01:01:24,242 - utils - INFO - Loading summarization model from ./models/summarizer_final\n","output_type":"stream"},{"name":"stdout","text":"Using GPU: Tesla T4\nGPU Memory: 15.64 GB\n","output_type":"stream"},{"name":"stderr","text":"2026-02-13 01:01:26,340 - utils - INFO - Summarization model loaded successfully\n2026-02-13 01:01:26,345 - utils - INFO - Loading Q&A model from ./models/qa_final\nDevice set to use cuda:0\n2026-02-13 01:01:26,501 - utils - INFO - Q&A model loaded successfully\n2026-02-13 01:01:26,501 - utils - INFO - System initialized successfully\n2026-02-13 01:01:26,502 - utils - INFO - Models loaded successfully\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"Using GPU: Tesla T4\nGPU Memory: 15.64 GB\n","output_type":"stream"},{"name":"stderr","text":"2026-02-13 01:01:29,097 - pyngrok.ngrok - INFO - Opening tunnel named: http-8000-0bac63ad-bc93-4cc4-8e3c-ee453c3b45dc\n2026-02-13 01:01:29,115 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"no configuration paths supplied\"\n2026-02-13 01:01:29,116 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n2026-02-13 01:01:29,118 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n2026-02-13 01:01:29,135 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"FIPS 140 mode\" enabled=false\n2026-02-13 01:01:29,142 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n2026-02-13 01:01:29,289 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n2026-02-13 01:01:29,291 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n2026-02-13 01:01:29,293 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=start pg=/api/tunnels id=dbb5e74e9a2b4d35\n2026-02-13 01:01:29,295 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=end pg=/api/tunnels id=dbb5e74e9a2b4d35 status=200 dur=238.967µs\n2026-02-13 01:01:29,297 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=start pg=/api/tunnels id=6eee3ee93135c008\n2026-02-13 01:01:29,298 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=end pg=/api/tunnels id=6eee3ee93135c008 status=200 dur=104.786µs\n2026-02-13 01:01:29,299 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=start pg=/api/tunnels id=cb0fdda31c9a238d\n2026-02-13 01:01:29,300 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=end pg=/api/tunnels id=cb0fdda31c9a238d status=200 dur=87.354µs\n2026-02-13 01:01:29,301 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=start pg=/api/tunnels id=28817ae3c099c4e9\n2026-02-13 01:01:29,328 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-0bac63ad-bc93-4cc4-8e3c-ee453c3b45dc addr=http://localhost:8000 url=https://2c31-34-61-103-96.ngrok-free.app\n","output_type":"stream"},{"name":"stdout","text":"\n✅ LIVE AT: NgrokTunnel: \"https://2c31-34-61-103-96.ngrok-free.app\" -> \"http://localhost:8000\"\n\n","output_type":"stream"},{"name":"stderr","text":"2026-02-13 01:01:29,330 - pyngrok.process.ngrok - INFO - t=2026-02-13T01:01:29+0000 lvl=info msg=end pg=/api/tunnels id=28817ae3c099c4e9 status=201 dur=30.232789ms\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ===================================\n# CELL: Direct Test - Upload a Real File\n# ===================================\n\nimport requests\nimport json\n\nBASE_URL = \"http://localhost:8000\"\n\n# Create a simple test PDF or TXT\ntest_content = \"\"\"\nThe Transformer model uses attention mechanisms.\nIt was introduced in 2017 by Vaswani et al.\nThe model achieves excellent results on translation tasks.\n\"\"\"\n\n# Save as text file\nwith open('simple_test.txt', 'w') as f:\n    f.write(test_content)\n\nprint(\"Testing with TXT file...\")\nwith open('simple_test.txt', 'rb') as f:\n    files = {'file': ('simple_test.txt', f, 'text/plain')}\n    data = {'questions': 'What is the Transformer?'}\n    \n    response = requests.post(f\"{BASE_URL}/upload-pdf\", files=files, data=data)\n    \n    print(f\"\\nStatus: {response.status_code}\")\n    if response.status_code == 200:\n        result = response.json()\n        print(f\"Success!\")\n        print(f\"Mode: {result.get('mode')}\")\n        print(f\"QA Results: {len(result.get('qa_results', []))} answers\")\n        if result.get('qa_results'):\n            for qa in result['qa_results']:\n                print(f\"\\nQ: {qa['question']}\")\n                print(f\"A: {qa['answer']}\")\n    else:\n        print(f\"Error: {response.text}\")\n\n# Now check what the frontend is sending\nprint(\"\\n\" + \"=\"*60)\nprint(\"Now test from the UI and paste the console logs here\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:29.333664Z","iopub.execute_input":"2026-02-13T01:01:29.334360Z","iopub.status.idle":"2026-02-13T01:01:29.364761Z","shell.execute_reply.started":"2026-02-13T01:01:29.334328Z","shell.execute_reply":"2026-02-13T01:01:29.364145Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 01:01:29,345 - utils - INFO - Received file: simple_test.txt, content_type: text/plain\n2026-02-13 01:01:29,346 - utils - INFO - Questions parameter: What is the Transformer?\n2026-02-13 01:01:29,346 - utils - INFO - Read 153 bytes\n2026-02-13 01:01:29,347 - utils - INFO - Processing as TXT...\n2026-02-13 01:01:29,348 - utils - INFO - Total extracted text: 153 characters\n2026-02-13 01:01:29,349 - utils - INFO - Q&A mode: processing 1 questions\n2026-02-13 01:01:29,359 - utils - INFO - Q&A completed: 1 answers in 0.01s\n","output_type":"stream"},{"name":"stdout","text":"Testing with TXT file...\nINFO:     127.0.0.1:59578 - \"POST /upload-pdf HTTP/1.1\" 200 OK\n\nStatus: 200\nSuccess!\nMode: qa\nQA Results: 1 answers\n\nQ: What is the Transformer?\nA: attention mechanisms\n\n============================================================\nNow test from the UI and paste the console logs here\n============================================================\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Step 10: Test API Endpoints\n\nOnce the server is running, you can test the endpoints:","metadata":{}},{"cell_type":"code","source":"import requests\n\nBASE_URL = \"http://localhost:8000\"  \n\n# Test summarization\nresponse = requests.post(\n    f\"{BASE_URL}/summarize\",\n    json={\"text\": sample_paper}\n)\nprint(\"Summarization result:\")\nprint(response.json())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:29.365703Z","iopub.execute_input":"2026-02-13T01:01:29.365990Z","iopub.status.idle":"2026-02-13T01:01:30.973426Z","shell.execute_reply.started":"2026-02-13T01:01:29.365959Z","shell.execute_reply":"2026-02-13T01:01:30.972777Z"}},"outputs":[{"name":"stdout","text":"INFO:     127.0.0.1:59592 - \"POST /summarize HTTP/1.1\" 200 OK\nSummarization result:\n{'summary': ' We propose a new simple network architecture, the Transformer, based solely on attention mechanisms. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. The Transformer follows the overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Our model uses multi-head attention mechanisms which allow the model to jointly attend to information from different representation subspaces. For each of these we use dk=dv=dmodel/h=64 dimensions.', 'input_length': 185, 'summary_length': 86}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Test Q&A\nresponse = requests.post(\n    f\"{BASE_URL}/ask\",\n    json={\n        \"question\": \"What is the main contribution?\",\n        \"context\": sample_paper\n    }\n)\nprint(\"Q&A result:\")\nprint(response.json())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:30.974438Z","iopub.execute_input":"2026-02-13T01:01:30.974774Z","iopub.status.idle":"2026-02-13T01:01:30.995517Z","shell.execute_reply.started":"2026-02-13T01:01:30.974739Z","shell.execute_reply":"2026-02-13T01:01:30.994833Z"}},"outputs":[{"name":"stdout","text":"INFO:     127.0.0.1:59594 - \"POST /ask HTTP/1.1\" 200 OK\nQ&A result:\n{'answer': 'draw global dependencies between input and output', 'score': 0.13593682646751404, 'start': 1008, 'end': 1057}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Step 11: Save Models for Later Use","metadata":{}},{"cell_type":"code","source":"# Models are already saved in ./models/\n# You can download them or upload to Kaggle Datasets\n\nimport shutil\n\n# Create archive of models\nshutil.make_archive('trained_models', 'zip', './models')\nprint(\"✓ Models archived as trained_models.zip\")\nprint(\"Download this file to save your trained models!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T01:01:30.996585Z","iopub.execute_input":"2026-02-13T01:01:30.996884Z","iopub.status.idle":"2026-02-13T01:01:46.206597Z","shell.execute_reply.started":"2026-02-13T01:01:30.996853Z","shell.execute_reply":"2026-02-13T01:01:46.205865Z"}},"outputs":[{"name":"stdout","text":"✓ Models archived as trained_models.zip\nDownload this file to save your trained models!\n","output_type":"stream"}],"execution_count":23}]}